{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AUD3_augmentation_data_deep_dive.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "owvJhH5Toz8O"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPrEkVuqF+rPC7w7h7XxgLJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duchaba/aud3_augmentation_data_deep_dive/blob/main/AUD3_augmentation_data_deep_dive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFr017_IS3RB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "bfa8a2cb-8df7-42a1-87f1-2c35fd0e374c"
      },
      "source": [
        "# OPTIONAL\n",
        "# For Google Colab, (1) Open the \"concole\", e.g. right-click and inspect, (2) Copy the below scripts (from line #10 to #17) and run it.\n",
        "#\n",
        "# If you know how to hack Google Colab Jupyter notebook and run \"javascripts\" as-is below, \n",
        "#i.e., without the need for opening up the console, please share it with me.\n",
        "#\n",
        "# The Javascript is to highlight the code cells' input and output and the code-cells you have executed.\n",
        "#\n",
        "%%js\n",
        "var head = document.head || document.getElementsByTagName(\"head\")[0];\n",
        "var style = document.createElement(\"style\");\n",
        "var css = \".inputarea.code{border-left: 4px solid #20c997;}.cell.focused .inputarea.code{border-left: 4px solid #d63384;}.cell .output{border-left: 4px solid #ffc107;}\";\n",
        "css = css + \":root { --colab-fresh-execution-count-color: #d63384;}\";\n",
        "css = css + \".markdown blockquote {border-left: 10px solid #fd7e14 !important;border-radius: 10px 0 0 10px;padding: 1em 1em 1em 1em;border-bottom: 1px solid #343845}\"\n",
        "css = css + \" h1,h2,h3,h4,h5 {font-family:serif !important;}\"\n",
        "css = css + \"h1{color:#e83e8c !important;;} h2{color:#20c997 !important;font-size:120%;} h3{color:#fd7e14 !important;font-size:120%;} h4{color:#6610f2 !important;}\"\n",
        "head.appendChild(style);\n",
        "style.type = \"text/css\";\n",
        "style.appendChild(document.createTextNode(css));\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "var head = document.head || document.getElementsByTagName(\"head\")[0];\n",
              "var style = document.createElement(\"style\");\n",
              "var css = \".inputarea.code{border-left: 4px solid #20c997;}.cell.focused .inputarea.code{border-left: 4px solid #d63384;}.cell .output{border-left: 4px solid #ffc107;}\";\n",
              "css = css + \":root { --colab-fresh-execution-count-color: #d63384;}\";\n",
              "css = css + \".markdown blockquote {border-left: 10px solid #fd7e14 !important;border-radius: 10px 0 0 10px;padding: 1em 1em 1em 1em;border-bottom: 1px solid #343845}\"\n",
              "css = css + \" h1,h2,h3,h4,h5 {font-family:serif !important;}\"\n",
              "css = css + \"h1{color:#e83e8c !important;;} h2{color:#20c997 !important;font-size:120%;} h3{color:#fd7e14 !important;font-size:120%;} h4{color:#6610f2 !important;}\"\n",
              "head.appendChild(style);\n",
              "style.type = \"text/css\";\n",
              "style.appendChild(document.createTextNode(css));"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMVB2r7SqHhq"
      },
      "source": [
        "# 1.0 |- Introduction | Augmentation Data Deep Dive (AUD3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--dFnTMyy-mg"
      },
      "source": [
        "Welcome to the “Augmentation Data Deep Dive” (AUD3) project. It is another journey in the “Demystify AI” series. \n",
        "\n",
        "The only thing these journeys have in common is the problems are taken from the real-world Artificial Neural Network (ANN) project. I have worked on them, coded them, cried over them, and sometimes had a flash-of-insight or an original thought about them.\n",
        "\n",
        "The journeys are a fun and fascinating insight into the daily working of an AI Scientist and Big-Data Scientist. They are for colleagues and AI students, but I hope that you, a gentle reader, would enjoy it too. \n",
        "\n",
        "The logic behind data augmentation is uncomplicated. You need more pictures to increase the ANN model accuracy, and data augmentation gives you more images. \n",
        "\n",
        "The AUD3 is a hackable, step-by-step Jupyter Notebook. It is for learning about data augmentation and selecting the correct parameters in the ANN image-classification and image-segmentation projects. You can skip ahead to the result-cells and not read the math and the code-cells. The math is elementary, and coding is straightforward logic, so try it out. You might enjoy “hacking” along the journey. \n",
        "\n",
        "Data augmentation increases the training images by a factor of 2 to 16 or more. For ANN, that means the model achieves better accuracy with more epoch and without over-fitting. \n",
        "\n",
        "For example, I was working on an AI project for a large casino hotel. The goal is to classify every customer into one of the 16 categories as they walk through the door. In other words, it is not to identify that guy walking through the door is “Duc Haba,” but to classify him as a “whale (A-1)” category, i.e., a big spender. \n",
        "\n",
        "As you have guessed, the first and most significant problem is the lack of labeled pictures. I need millions of tagged photos because of human diversity in race, ethnicity, clothing, different camera angle, and so on. \n",
        "\n",
        "ANN is not a ruled-based expert system. For example, a person wearing a Rolex watch is an “A-1”, or a guy with no shoe and no shirt is a “D-4” category. ANN does not use rules, so it needs millions of labeled images to train and to generalize so the ANN model can classify a guy who enters the casino for the first time correctly. In ANN’s lingual, it means the ANN model is not over-fitting. \n",
        "\n",
        "I classify the AUD3 as a \"sandbox\" project. In other words, it is a fun, experimental project focusing on solving one problem.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VRysR3wzHAV"
      },
      "source": [
        "><center><h2><i>So if you are ready, let's take a collective calming breath …  … and begin.</i></h2></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgcUkDe42hm8"
      },
      "source": [
        "# 2.0 |- The Journey\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kiJpsp96UKw"
      },
      "source": [
        "- As with the previous journey, the first step is to choose and name your dog companion. With the project code name AUD3, the natural name has to be “Wallaby.” \n",
        "\n",
        "- Typically a dog name is chosen, e.g., \"Lefty,\" \"Roosty,\" or \"Eggna,\" but be warned, don't name it after your cat because a \"cat\" will not follow any commands.\n",
        "\n",
        "- If you are serious about learning augmenting data, start hacking by changing the companion name to your preference. Jupyter notebook allows you to add notes, add new codes, and hack Wallaby’s code. \n",
        "\n",
        "- If you are a friend tagging along, you will like Wallaby. He is a friendly, helpful dog. He will do all the tedious work in good spirits, and he likes to hop around. \n",
        "\n",
        "- As a good little programmer, Wallaby (or insert your companion name here) starts by creating an object or class.\n",
        "\n",
        "![wallaby](https://github.com/duchaba/aud3_augmentation_data_deep_dive/blob/main/wallaby3.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoKldkXvDud5"
      },
      "source": [
        "## 2.1 | Wallaby's \"River\" Coding Style"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPVQ9uY3ESF1"
      },
      "source": [
        "- Wallaby uses the \"river\" coding style.\r\n",
        "\r\n",
        "- The style uses a full library name, sub-class, and following by the function name. Jupyter notebook has auto-complete, so Wallaby would not misspell long variable and function name. \r\n",
        "\r\n",
        "- Wallaby is NOT using the global-space, such as \"import numpy *\" or using the shorten name like \"import matplotlib.pyplot as ptl.” Instead he is using the full {river} name as in “numpy.random.randint().”\r\n",
        "\r\n",
        "- Furthermore, Wallaby shies away from using Python language-specific syntax shorthand, such as the “assigned if” statement construct. He likes to write the Python code using standard Python libraries and not relying on exotic packages. \r\n",
        "\r\n",
        "- The primary reason for using the “river” coding style coupled with a descriptive naming convention is that it is easier to read, hack, and translate to Swift or Javascript.\r\n",
        "\r\n",
        "- For any sandbox project, Wallaby is in the exploration mode, and therefore, he will refactor and optimize the code afterward. When Wallaby using [Atom IDE](htpps://atom.io) to upload the code to GitHub, he may refactor them to make them run faster, but not for syntax compaction or syntax shorthand. \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ6Bq8NpE2zg"
      },
      "source": [
        "## 2.2 | Wallaby Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7YSz_fD2pwF"
      },
      "source": [
        "# import standard libraries\n",
        "import numpy\n",
        "import pathlib\n",
        "import os\n",
        "import pandas\n",
        "import matplotlib"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCBu7s0Ky5C2"
      },
      "source": [
        "class AUD3(object):\n",
        "  #\n",
        "  # initialize the object\n",
        "  def __init__(self, name=\"Wallaby\"):\n",
        "    self.author = \"Duc Haba\"\n",
        "    self.name = name\n",
        "    self._ph()\n",
        "    self._pp(\"Hello from\", self.__class__.__name__)\n",
        "    self._pp(\"Code name\", self.name)\n",
        "    self._pp(\"Author is\", self.author)\n",
        "    self._ph()\n",
        "    return\n",
        "  #\n",
        "  # pretty print output name-value line\n",
        "  def _pp(self, a, b):\n",
        "    print(\"%40s : %s\" % (str(a), str(b)))\n",
        "    return\n",
        "  #\n",
        "  # pretty print the header or footer lines\n",
        "  def _ph(self):\n",
        "    print(\"-\" * 40, \":\", \"-\" * 40)\n",
        "    return\n",
        "  #\n",
        "  # dance\n",
        "  def dance_happy(self):\n",
        "    char = \"        _=,_\\n    o_/6 /#\\\\\\n    \\\\__ |##/\\n     ='|--\\\\\\n       /   #'-.\\n\"\n",
        "    char = char + \"       \\\\#|_   _'-. /\\n        |/ \\\\_( # |\\\" \\n       C/ ,--___/\\n\"\n",
        "    print(char)\n",
        "    self._ph()\n",
        "    return\n",
        "# ---end of AUD3 class"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI5Zh7rly6um",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e04b6002-d76f-4a45-bda4-951a7261dcb5"
      },
      "source": [
        "# Let start\r\n",
        "wallaby = AUD3(\"Wallaby\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------- : ----------------------------------------\n",
            "                              Hello from : AUD3\n",
            "                               Code name : Wallaby\n",
            "                               Author is : Duc Haba\n",
            "---------------------------------------- : ----------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpWVZ7BmxJdL",
        "outputId": "94777c9b-ad7a-4c1c-e91c-abe21718f098"
      },
      "source": [
        "# dance baby dance\r\n",
        "wallaby.dance_happy()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        _=,_\n",
            "    o_/6 /#\\\n",
            "    \\__ |##/\n",
            "     ='|--\\\n",
            "       /   #'-.\n",
            "       \\#|_   _'-. /\n",
            "        |/ \\_( # |\" \n",
            "       C/ ,--___/\n",
            "\n",
            "---------------------------------------- : ----------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZKa53hWDgTa"
      },
      "source": [
        "- The following is a clean version. Wallaby cleans up the tried-and-errors cells, but please don't let it stop you from inserting your code-cells and notes as we make this journey together. \n",
        "\n",
        "- When copying the code into the Atom's project, Wallaby would add the methods during the class definition, but in a notebook, he will hack-it and add new functions as need it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ub_Aq254HKMw"
      },
      "source": [
        "# Hack it!\n",
        "# add_method() is copy from Michael Garod's blog, \n",
        "# https://medium.com/@mgarod/dynamically-add-a-method-to-a-class-in-python-c49204b85bd6\n",
        "# AND correction by: Филя Усков\n",
        "#\n",
        "import functools\n",
        "def add_method(cls):\n",
        "  def decorator(func):\n",
        "    @functools.wraps(func) \n",
        "    def wrapper(self, *args, **kwargs): \n",
        "      return func(self,*args, **kwargs)\n",
        "    setattr(cls, func.__name__, wrapper)\n",
        "    return func # returning func means func can still be used normally\n",
        "  return decorator"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWsn6LOFNOqH"
      },
      "source": [
        "## 2.3 | Detour to Find Our Friend Monty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyuUcPRAOZcg"
      },
      "source": [
        "- Monty is like Wallaby. He is a Python class refactored in the Atom project and stored in GitHub. \n",
        "\n",
        "- Monty is an alpha-dog, and therefore, he follows the same methodology. Hacked it in a Jupyter notebook and then copy and refactor in a Python Atom project.\n",
        "\n",
        "- Monty is not a public Github project at this stage. However, Monty's code exists in many of Duc Haba's sandbox projects on Github.\n",
        "\n",
        "- Monty uses \"[fast.ai](https://fasta.ai)\" library version 1.0.62.x from Jeremy Howard, Rachel Thomas, and Sylvain Gugger. Fast.ai library uses PyTorch version 1.6.x and Python 3.6.9.\n",
        "\n",
        "- For this journey, Monty ability to draw 2D, 3D graphs, and image-cleaning will be handy. They were from previous journies, the \"[Demystify Python 2D Charts](https://www.linkedin.com/pulse/demystify-python-charts-hackable-step-by-step-jupyter-duc-haba/),\" and the \"[3D Visualization](https://www.linkedin.com/pulse/python-3d-visualization-hackable-step-by-step-jupyter-duc-haba/)\" sandbox projects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_lIfkqBK3iG"
      },
      "source": [
        "%%capture out_1\n",
        "# load in fastai and pytorch. It is optional if are coding on your labtop\n",
        "# load in fastai at May 1 2020 version\n",
        "!pip install --upgrade git+https://github.com/duchaba/fastai.git\n",
        "# !pip install --upgrade git+https://github.com/duchaba/monty_NOT_AVAILABLE\n",
        "\n",
        "# import Monty and create a monty instant. The preference is NOT using global space\n",
        "import d0hz.fastai_util \n",
        "monty = d0hz.fastai_util.base_monty()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5YndkRYWCn9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a37344a9-adf3-4a91-df0e-bb7b31dcb441"
      },
      "source": [
        "# double checked\n",
        "monty.print.sys_info()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------- : ----------------------------------------\n",
            "                             System time : 2020/12/10 21:56\n",
            "                                Platform : linux\n",
            "                          Python version : 3.6.9 (default, Oct  8 2020, 12:12:24) \n",
            "[GCC 8.4.0]\n",
            "                         PyTorch version : 1.7.0+cu101\n",
            "                     Fastai version is:  : 1.0.62.dev0\n",
            "                           Monty version : 0.6.0\n",
            "                               CPU count : 4\n",
            "                              *CPU speed : NOT available\n",
            "                               RAM total : 25.51 Gb\n",
            "                                RAM free : 24.46 Gb, 95.9%\n",
            "                                GPU-Cuda : True\n",
            "                        Disk space total : 147.15 Gb\n",
            "                         Disk space free : 114.95 Gb, 78.1%\n",
            "                      Current directory: : /content\n",
            "             Python import packages path : Full path below...\n",
            "                                       + : \n",
            "                                       + : /env/python\n",
            "                                       + : /usr/lib/python36.zip\n",
            "                                       + : /usr/lib/python3.6\n",
            "                                       + : /usr/lib/python3.6/lib-dynload\n",
            "                                       + : /usr/local/lib/python3.6/dist-packages\n",
            "                                       + : /usr/lib/python3/dist-packages\n",
            "                                       + : /usr/local/lib/python3.6/dist-packages/IPython/extensions\n",
            "                                       + : /root/.ipython\n",
            "---------------------------------------- : ----------------------------------------\n",
            "                           Total GPU RAM :  15.90 GB\n",
            "                            Free GPU RAM :  15.89 GB\n",
            "                            Free CPU RAM :  24.46 GB\n",
            "            Garbage collection recovered :  28.00\n",
            "---------------------------------------- : ----------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gt_153DNVNt"
      },
      "source": [
        "## 2.4 | Fetch Images Data\n",
        "\n",
        "- Wallaby has a companion named Monty. He will do all the dirty works that do not directly pertain to this journey. If we spend time teaching Wallaby, then it will distract from the AUD3 journey.\n",
        "\n",
        "- Wallaby encourages you to hack the notebook, and use your image data set.\n",
        "\n",
        "- His first task is as follows.\n",
        "\n",
        "1. Fetch the farm animal image set.\n",
        "\n",
        "2. Fetch the city image set.\n",
        "\n",
        "3. Fetch the people faces image set.\n",
        "\n",
        "4. Fetch the satellite image set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57-DZSqSI-bu"
      },
      "source": [
        "- Wallaby randomly pulls the images from \"Google\" or \"Bing\" image-searches. He uses the Chrome extention \"abc\" to aid in downloading and packed them into a zip-file. \r\n",
        "\r\n",
        "- Wallaly claims <u>no rights</u> on these pictures. He uses them only for research purposes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oiI90R-LXEN"
      },
      "source": [
        "import requests\n",
        "import zipfile\n",
        "# fetch data from url\n",
        "@add_method(AUD3)\n",
        "def _fetch_external_file(self,src_url, dst_path):\n",
        "  ext_file = requests.get(src_url, allow_redirects=True)\n",
        "  self._pp(\"Response Status Code \" + str(ext_file.status_code), ext_file.reason)\n",
        "  local_file = open(dst_path,mode=\"wb\")\n",
        "  local_file.write(ext_file.content)\n",
        "  local_file.close()\n",
        "  return dst_path\n",
        "#\n",
        "#\n",
        "# unzip file\n",
        "@add_method(AUD3)\n",
        "def _unpack_zipfile(self, src,dst):\n",
        "  with zipfile.ZipFile(src, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(path=dst)\n",
        "  return\n",
        "#\n",
        "#\n",
        "# fetch data\n",
        "@add_method(AUD3)\n",
        "def fetch_data(self):\n",
        "  #set up\n",
        "  self.img_ext_faces_url = \"https://github.com/duchaba/aud3_augmentation_data_deep_dive/blob/main/scratch/faces.zip?raw=True\"\n",
        "  # create the \"data\"\n",
        "  self.data_path = pathlib.Path(\"data\")\n",
        "  if os.path.isdir(self.data_path) == False:\n",
        "    os.mkdir(self.data_path)\n",
        "  dst = self.data_path.joinpath(\"faces.zip\")\n",
        "  # header\n",
        "  self._ph()\n",
        "  self._pp(\"Fetch\", \"Image data sets\")\n",
        "  self._pp(\"Destination: \"+str(dst), \"Source: \" + self.img_ext_faces_url)\n",
        "  # fetch faces\n",
        "  self.img_path = self.data_path.joinpath(\"img\")\n",
        "  self._fetch_external_file(self.img_ext_faces_url,dst)\n",
        "  self._unpack_zipfile(dst,self.img_path)\n",
        "  self._pp(\"Unpack \"+str(dst) + \" at\", self.img_path)\n",
        "  # fetch cityscape\n",
        "  self.img_ext_cityscape_url = \"https://github.com/duchaba/aud3_augmentation_data_deep_dive/blob/main/scratch/cityscape.zip?raw=True\"\n",
        "  dst = self.data_path.joinpath(\"cityscape.zip\")\n",
        "  self._pp(\"Destination: \"+str(dst), \"Source: \" + self.img_ext_cityscape_url)\n",
        "  self._fetch_external_file(self.img_ext_cityscape_url,dst)\n",
        "  self._unpack_zipfile(dst,self.img_path)\n",
        "  self._pp(\"Unpack \"+str(dst) + \" at\", self.img_path)\n",
        "  #\n",
        "  # fetch landscape\n",
        "  self.img_ext_landscape_url = \"https://github.com/duchaba/aud3_augmentation_data_deep_dive/blob/main/scratch/landscape.zip?raw=True\"\n",
        "  dst = self.data_path.joinpath(\"landscape.zip\")\n",
        "  self._pp(\"Destination: \"+str(dst), \"Source: \" + self.img_ext_landscape_url)\n",
        "  self._fetch_external_file(self.img_ext_landscape_url,dst)\n",
        "  self._unpack_zipfile(dst,self.img_path)\n",
        "  self._pp(\"Unpack \"+str(dst) + \" at\", self.img_path)\n",
        "  #\n",
        "  self._ph()\n",
        "  return "
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAaJDqkJHPvp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a52dfc4-a372-444a-e58e-463dc26205c7"
      },
      "source": [
        "# do it\r\n",
        "wallaby.fetch_data()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------- : ----------------------------------------\n",
            "                                   Fetch : Image data sets\n",
            "             Destination: data/faces.zip : Source: https://github.com/duchaba/aud3_augmentation_data_deep_dive/blob/main/scratch/faces.zip?raw=True\n",
            "                Response Status Code 200 : OK\n",
            "                Unpack data/faces.zip at : data/img\n",
            "         Destination: data/cityscape.zip : Source: https://github.com/duchaba/aud3_augmentation_data_deep_dive/blob/main/scratch/cityscape.zip?raw=True\n",
            "                Response Status Code 200 : OK\n",
            "            Unpack data/cityscape.zip at : data/img\n",
            "         Destination: data/landscape.zip : Source: https://github.com/duchaba/aud3_augmentation_data_deep_dive/blob/main/scratch/landscape.zip?raw=True\n",
            "                Response Status Code 200 : OK\n",
            "            Unpack data/landscape.zip at : data/img\n",
            "---------------------------------------- : ----------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8icFP-hKW8i"
      },
      "source": [
        "- That was easy-peazy-lemon-squeezy. Wallaby is a good fetching dog. Imagine if you choose a cat as your companion on this journey. First, a cat will not listen to the command \"fetch,\" and if a merical-of-merical happens, a cat will most likely coming back with a dead bird. It is because a cat always do what he wants to do."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArxoO5DhHPzQ"
      },
      "source": [
        "@add_method(AUD3)\n",
        "def print_readme(self):\n",
        "  self.readme = pathlib.Path(\"data/imdb/README\")\n",
        "  monty.print.text_file(self.readme, max_line_display=100) # all of it\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dKYQGiGHP28"
      },
      "source": [
        "Wallaby.print_readme()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPlPYf30Ta_8"
      },
      "source": [
        "- Before diving back in, the IMDB-data is one of a more well-organized data set. Most real-world projects are not that lucky.\n",
        "\n",
        "- The \"README\" file confirmed Wallaby’s initial observations. Wallaby also learns a few more tidbits. Which to say, if you have a \"README\" file, then read it first.\n",
        "\n",
        "- There is a rating number for each review embedded in the filename. It ranges from 1 to 10, i.e., 1 to 5 stars. For example, rating three is 1.5 stars, nine is 4.5 stars, and ten is five stars rating.\n",
        "\n",
        "- It is a fantastic insight because after Wallaby completes the NLP sentiment identification model, \"positive and negative,\" she can train the NLP model to rate the movie reviews. Wallaby has a chance to do an off-book project.\n",
        "\n",
        "- The \"unsup\" is the \"unsupervised\" reviews, i.e., movie reviews without a label.\n",
        "\n",
        "- When training NLP using Convolutional Neural Network (CNN), there are two sets of data-bunch required. The first train session teaches the CNN model to predict the next words. We will include the files in the \"unsup\" directory, and the second train session instructs the CNN model to identify the \"positive and negative\" sentiment.\n",
        "\n",
        "- The third NLP training session is Wallaby’s “product rating review” off-book idea. In other words, she will train the NLP model to predict one to five stars for movie reviews.\n",
        "\n",
        "- In doing the next-word NLP training session, she will create the vocabulary file, and therefore, she does not need the \"imdb.vocab\" and the \"*.feat\" files.\n",
        "\n",
        "- Most of Wallaby’s friends jump head-first in creating the data-bunch at this stage in the journey. They start the training session and fuzz over the hyper-parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHPRJ-c4y8JX"
      },
      "source": [
        "Wallaby._ph()\n",
        "Wallaby._pp(\"Wallaby\", \"Slowing down and drawing graphs.\")\n",
        "Wallaby._ph()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvJLdesWERwX"
      },
      "source": [
        "## 2.5 Clean The Input-data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVKSfJktd3TI"
      },
      "source": [
        "- Before coding and tokenizing the data-bunch, Wallaby will clean up the data.\n",
        "\n",
        "1. Move all unnecessary files and directories into a “scratch” directory. In other words, keep only the “*.txt” files.\n",
        "\n",
        "2. Remove all HTML-tags or unprintable characters from the movie reviews.\n",
        "\n",
        "3. Remove any movie reviews that have more than 1,100 words. It could be contentious, but from the graphs, there are about 50 reviews in each category that were too long, and we have 25,000 files in the “train” and “test” directories and 50,000 in the “unsup” directory. Furthermore, the average word count is 236, so chop away.\n",
        "\n",
        "- Normally, Wallaby uses the 80-20 rules, i.e., 80% of data for training and 20% for validation. We have 50% of data for the “test,” and that is too much. Wallaby would like to split 75-15-10, i.e., 75% for train-set, 15% for validation-set, and 10% for test-set. There is a total of 50,000 labeled movie reviews.\n",
        "\n",
        "- Before you cry foul, you are not allowed to change the “test” set in a competition, so by doing this, the competition might disqualify Wallaby, but who could be mean to a happy, tail-waggy dog. You can’t say “no” to Wallaby.\n",
        "\n",
        "- The AUD3 journey is about demystifying NLP data and not so much about Kaggle's competition. Therefore, we will follow Wallaby’s 75-15-10 suggestion. \n",
        "\n",
        "- It is a logical split, and it does not violate the rule. The train-set has a separate validation-set, and the test-set is unused in the training session.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGFBKbH0SORL"
      },
      "source": [
        "# clean files\n",
        "@add_method(AUD3)\n",
        "def clean_files(self):\n",
        "  # creat scratch dir\n",
        "  self.scratch_dir = pathlib.Path(\"data/scratch\")\n",
        "  if os.path.isdir(self.scratch_dir) == False:\n",
        "    os.mkdir(self.scratch_dir)\n",
        "  # move the tmp dir\n",
        "  src = pathlib.Path(\"data/imdb/tmp_lm\")\n",
        "  dst = pathlib.Path(self.scratch_dir, \"/tmp_lm\")\n",
        "  os.rename(src,dst)\n",
        "  src = pathlib.Path(\"data/imdb/tmp_clas\")\n",
        "  dst = pathlib.Path(self.scratch_dir, \"/tmp_clas\")\n",
        "  os.rename(src,dst)\n",
        "  # ask Monty to move non-*.txt files to scratch\n",
        "  src = pathlib.Path(\"data/imdb\")\n",
        "  dst = self.scratch_dir\n",
        "  monty.clean.unwanted_files(src, \".txt\", scratch_dir=dst, is_inversed=True)\n",
        "  # ask Monty to clean html-tag\n",
        "  monty.clean.html_tags_all(src)\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2biEeBZdSOJv"
      },
      "source": [
        "Wallaby._ph()\n",
        "Wallaby._pp(\"Clean files\", \"Remove non-text files, remove HTML-tag and non-printable char.\")\n",
        "Wallaby.clean_files()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5_RqrCeSOGm"
      },
      "source": [
        "# clean long files\n",
        "@add_method(AUD3)\n",
        "def clean_long_files(self,max_wc=1100):\n",
        "  src = pathlib.Path(\"data/imdb\")\n",
        "  i = 0\n",
        "  for root, dirs, files in os.walk(src):  # @UnusedVariable\n",
        "    for name in files:\n",
        "      a = pathlib.Path(root, name)\n",
        "      wc = self._fetch_words_in_file(a)\n",
        "      if (wc > max_wc):\n",
        "        dst = pathlib.Path(self.scratch_dir, name)\n",
        "        os.rename(a,dst)\n",
        "        i = i + 1\n",
        "  self._ph()\n",
        "  self._pp(\"Clean files\", \"Remove movie reviews longer than 1,100 word count.\")\n",
        "  self._pp(\"Total delete file count\", i)\n",
        "  self._ph()\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1NHQ41xjNzA"
      },
      "source": [
        "# do it\n",
        "Wallaby.clean_long_files()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_ZjdfLDjN3e"
      },
      "source": [
        "# clean 75-15-10\n",
        "@add_method(AUD3)\n",
        "def clean_75_15_10(self):\n",
        "  # set up creat scratch dir\n",
        "  self.valid_dir = pathlib.Path(\"data/imdb/valid\")\n",
        "  if os.path.isdir(self.valid_dir) == False:\n",
        "    os.mkdir(self.valid_dir)\n",
        "    pos = self.valid_dir.joinpath(\"pos\")\n",
        "    os.mkdir(pos)\n",
        "    neg = self.valid_dir.joinpath(\"neg\")\n",
        "    os.mkdir(neg)\n",
        "  self.train_dir = pathlib.Path(\"data/imdb/train\")\n",
        "  self.test_dir = pathlib.Path(\"data/imdb/test\")\n",
        "  # ask Monty to do the heavy lifting\n",
        "  # valid 15% total or 30% files in \"test\" dir\n",
        "  monty.clean.split_files_by_perc(self.test_dir.joinpath(\"pos\"),pos, perc=0.30)\n",
        "  monty.clean.split_files_by_perc(self.test_dir.joinpath(\"neg\"),neg, perc=0.30)\n",
        "  # test is 10% total or 20% files in \"test\" dir. \n",
        "  # quick math show that we need to move the remaining in \"test\" by 0.7143 to train\n",
        "  monty.clean.split_files_by_perc(self.test_dir.joinpath(\"pos\"), self.train_dir.joinpath(\"pos\"), perc=0.7143, head=\"t\")\n",
        "  monty.clean.split_files_by_perc(self.test_dir.joinpath(\"neg\"), self.train_dir.joinpath(\"neg\"), perc=0.7143, head=\"t\")\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHsAmvY2jN-m"
      },
      "source": [
        "Wallaby._ph()\n",
        "Wallaby._pp(\"Clean files\", \"The 75-15-10 ratio\")\n",
        "Wallaby.clean_75_15_10()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzKqdTvYOgct"
      },
      "source": [
        "# double checked\n",
        "monty.print.dir_tree(\"data\", max_file_display=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He7G66x8i4jq"
      },
      "source": [
        "- Wallaby loves to draw, so humors her and asks her to draw a few bar charts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvR6A8eNjOM8"
      },
      "source": [
        "# count files\n",
        "@add_method(AUD3)\n",
        "def _fetch_count_files(self, src_dir):\n",
        "  i = 0\n",
        "  for root, dirs, files in os.walk(src_dir):  # @UnusedVariable\n",
        "      for name in files:\n",
        "        i += 1\n",
        "  return i\n",
        "#\n",
        "#\n",
        "# draw the train, test, valid data set\n",
        "@add_method(AUD3)\n",
        "def draw_data_set(self):\n",
        "  # set up\n",
        "  mx_data = numpy.ones((3,2))\n",
        "  mx_data[:,0] = numpy.arange(1,4)\n",
        "  mx_data[0,1] = self._fetch_count_files(self.train_dir)\n",
        "  mx_data[1,1] = self._fetch_count_files(self.valid_dir)\n",
        "  mx_data[2,1] = self._fetch_count_files(self.test_dir)\n",
        "  # draw bar chart\n",
        "  frame, pic = monty.fetch.graph_canvas()\n",
        "  monty.draw.graph_bar(pic,mx_data, is_hand=True,color=monty.bag.color.pink)\n",
        "  monty.draw.graph_label(pic,xlabel=\"Train, Validate, and Test Data Set\", ylabel=\"Word Count\", head=\"The 75-15-10 Split\")\n",
        "  frame.show()\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e82zFiHjORn"
      },
      "source": [
        "# Wallaby, draw away\n",
        "Wallaby.draw_data_set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owvJhH5Toz8O"
      },
      "source": [
        "## 2.6 The Fast.ai Standard Six Steps for Creating the \"Next Word Prediction\" Data-bunch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5kV-2LxoXGF"
      },
      "source": [
        "- The hand-drawn “75-15-10 split” bar chart is beautiful. \n",
        "\n",
        "- Moving ahead, Wallaby creates the data-bunch for \"next word prediction\" and \"sentiment prediction.\"\n",
        "\n",
        "- The process is that Wallaby does one-step and inspects it. The process is the same for NLP as for image classification. The six-steps are as follows.\n",
        "\n",
        "1. Read (or input) the data and inspect it.\n",
        "\n",
        "2. Split the “train” and “valid” set and inspect it.\n",
        "\n",
        "3. Label the data and inspect it. For NLP, this step includes the tokenizer.\n",
        "\n",
        "4. Add data augmentation and inspect it. Wallaby will skip this step for NLP.\n",
        "\n",
        "5. Fetch the batch-size, the data-bunch, and inspect it.\n",
        "\n",
        "6. Normalize the input-data for the selected base-architecture. Wallaby will skip this step for NLP.\n",
        "\n",
        "  - Wallaby relies on Fast.ai library and Monty to do the heavy lifting. If we dive deep into writing the code from scratch, it will distract from the “AUD3” journey. Moreover, Fast.ai libraries are the best artificial neural network (ANN) libraries. It is far superior to Tensor or Keras.\n",
        "\n",
        "  - Incidentally, Wallaby stumbled on a salient factor for using the Jupyter notebook. If you disagree, you can “hack it.”\n",
        "\n",
        "  - If you think Tensor or any other ANN libraries are better, then you must hack this notebook. Wallaby and I welcome and encourage you to hack the notebook.\n",
        "\n",
        "  - **If you can’t hack-it, you won’t make-it. :-)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shVujVT5eaUf"
      },
      "source": [
        "### 2.6.1 Read (or input) the data and inspect it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q50GujLQje43"
      },
      "source": [
        "import fastai\n",
        "import fastai.text\n",
        "#\n",
        "@add_method(AUD3)\n",
        "def fetch_1of6_read_data(self):\n",
        "  self.data_path = pathlib.Path(\"data/imdb\")\n",
        "  self._nlp_1of6 = fastai.text.TextList.from_folder(self.data_path)\n",
        "  monty.print.inspect_fastai_textlist(self._nlp_1of6)\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2YNJvPmje-P"
      },
      "source": [
        "# do it\n",
        "Wallaby.fetch_1of6_read_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3GYgsAf2fn2"
      },
      "source": [
        "- That looks too easy. The folks from Fast.ai deserve big applause to “make AI cool again.”\n",
        "\n",
        "- Wallaby read in almost one hundred thousand movie reviews, 99,944, which is correct because she decided to delete the \"too long\" reviews. Also, the movie reviews are clear of HTML-tag and non-printing characters.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJlduOwResTa"
      },
      "source": [
        "### 2.6.2 Split the “train” and “valid” set and inspect it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEnwSeqUje2Q"
      },
      "source": [
        "# split 85/15\n",
        "@add_method(AUD3)\n",
        "def fetch_2of6_split_data(self):\n",
        "  self._nlp_2of6 = self._nlp_1of6.split_by_rand_pct(0.15)\n",
        "  monty.print.inspect_fastai_itemlists(self._nlp_2of6)\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycmhiVhG31ES"
      },
      "source": [
        "# do it\n",
        "Wallaby.fetch_2of6_split_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xD2Cvr26Nos"
      },
      "source": [
        "- So why does Wallaby NOT splits by the “train” and “valid” directories? It is because during the first training session, she is trying to predict the next word, and she uses the “unsupervised” data set. In the second data-bunch, Wallaby will split by the “train” and “valid” directories.\n",
        "\n",
        "- Once again, the code looks too easy. It is because the Fast.ai libraries do extensive work under the hood.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-yr_8AVfUXK"
      },
      "source": [
        "### 2.6.3 Label and tokenize the data and inspect it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaKR70J331Jq"
      },
      "source": [
        "# label for language model\n",
        "@add_method(AUD3)\n",
        "def fetch_3of6_label_data(self):\n",
        "  self._nlp_3of6 = self._nlp_2of6.label_for_lm()\n",
        "  monty.print.inspect_fastai_labellists(self._nlp_3of6 )\n",
        "  return\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGPy4atIjey3"
      },
      "source": [
        "# do it\n",
        "Wallaby.fetch_3of6_label_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-_ZmGK2Jnuj"
      },
      "source": [
        "- Wallaby gives too much information to digest, so she will take it one at a time.\n",
        "\n",
        "1. In the output \"Section #1\" above, the 75-15 split between “train” (84,953 files) and “valid” (14,991 files) is good.\n",
        "\n",
        "2. In \"Section #2\", the movie reviews are converted to tokens.\n",
        "\n",
        "3. At first glance, the first 20 tokens of a file are correct.\n",
        "\n",
        "4. In \"Section #3\", the total tokens count is 163,207. That is too many tokens because, from the IMDB vocab file, there should be 89,527 tokens. It is almost twice as many tokens. What’s going on?\n",
        "\n",
        "5. Wallaby shows the first 20 tokens. The “xx-” tokens are special tokens.\n",
        "\n",
        "6. In \"Section #4\", Wallaby shows the last 20 tokens, which marked as “zero” or “xxunk” mean unknown. There are misspelled words or make-up words, but we have to dive deep into the unused words.\n",
        "\n",
        "7. In \"Section #5 and #6\", Fast.ai default to 60,000 tokens. That means the system discards 103,207 words, which is throw out 63% of the text in the movie reviews. It’s not good. Wallaby should rework our data or deep dive into the Fast.ai library to fix it.\n",
        "\n",
        "8. There is no easy way to do this or any fancy graphs that would shorten the time. Wallaby has to dig deep. It is good that Wallaby is a dog and not a cat. :-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g82GqP3Sf3z_"
      },
      "source": [
        "### 2.6.3B -- Dig Deep in Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKjgcqQt6Me_"
      },
      "source": [
        "import textwrap\n",
        "# print out random 200 unused words, max_limit (60,000) is from fast.ai\n",
        "@add_method(AUD3)\n",
        "def print_random_unused_words(self, wc=200, max_limit=60000, is_reversed=False):\n",
        "  k = list(self._nlp_3of6.vocab.stoi.keys())\n",
        "  v = list(self._nlp_3of6.vocab.stoi.values())\n",
        "  max = len(k)\n",
        "  self._ph()\n",
        "  if is_reversed:\n",
        "    i = numpy.random.randint(1, high=(max_limit-wc))\n",
        "    self._pp(\"Random Good Words, Tokens, count\", wc)\n",
        "  else:\n",
        "    i = numpy.random.randint(max_limit, high=(max-wc))\n",
        "    self._pp(\"Random Unknown, Unused Words, count\", wc)\n",
        "  self._pp(\"Index\", i)\n",
        "  j = i + wc\n",
        "  self._ph()\n",
        "  words = str(k[i:j])\n",
        "  print(textwrap.fill(words,width=80))\n",
        "  self._ph()\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPZCd7rD6MbI"
      },
      "source": [
        "#print random unknown, unused words, run it a dozen of time or more.\n",
        "Wallaby.print_random_unused_words()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XE2Omycz-wYR"
      },
      "source": [
        "#print random valid token\n",
        "Wallaby.print_random_unused_words(is_reversed=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnrXYrU9ZE1p"
      },
      "source": [
        "- After running the “print_random_unused_words” code-cell a dozen times or more, Wallaby has a few recommendations to improve the tokenizer process.\n",
        "\n",
        "1. There are many combined words with a “period without space,” such as “somewhat.like,” “like.that,” “floor.what,” “drunk.and,” “score.well.defies,” “as.well.something,” or “convincing.all.”\n",
        "\n",
        "2. Similar to the above, combined words with a “comma without space” and “dash without space” is the problem.\n",
        "\n",
        "3. By purposely discarding the misspelled words, the system bias against reviewers who can’t spell. In other words, if the users are lazy and a lousy speller, e.g., write a ten words review with five misspelled words, the NLP model will not be able to predict it correctly.\n",
        "\n",
        "4. No one worried about biases in a Kaggle competition, but one should speak out about the intentional biases in a real-world project.\n",
        "\n",
        "5. For example, if the NLP is used to monitor and recommend your newsfeed and advertising-feed and doesn’t correct your spelling, the system will classify you with a false persona.\n",
        "\n",
        "6. Wallaby is a dog, but she loved to read science fiction stories. In her story, a government drone using NLP to grant entrance to the castle. It’s a very dull and desolate castle because the drone gave only privileged lawyers and English-major students access. :-)\n",
        "\n",
        "7. Wallaby can choose to include misspelled words or make-up words. The NLP model does what she told it to do. In other words, the NLP model can be used to write like a Democrat and not Republican, write like a Nobel laureate, or in the opposite end, write like Batman's Joker. It could be propaganda or fake news, but the salient point is that the decision which words to tokenized or discarded contributes substantially to the NLP model or adversely affects the accuracy with intentional biases.\n",
        "\n",
        "8. Furthermore, if you read the NLP’s valid words, some spell incorrectly. The algorithm is if the term is used twice and the maximum buffer has not exceeded, then count the word as valid.\n",
        "\n",
        "9. Wallaby read the valid token by setting the parameter “is_reversed=True.” There are many misspelled words and make-up words. In other words, if a couple of people write “Kastle instead of Castle” and “Kastle” appear earlier in the tokenizer process, then “Kastle” is valid. There is no grammar checking in the tokenizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZFxHMSOgKcz"
      },
      "source": [
        "### 2.6.3C -- Tokinizer Graphs, an Original Thought"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L86bI0yvlkbj"
      },
      "source": [
        "- There are too much data to ingest. Wallaby is a dog. She can’t count past five, and I can’t do much better. Therefore, we draw graphs. \n",
        "\n",
        "- There are 100,000 files, and each file has on the average 200 words, so that is 20 million tokens. I have read extensively in NLP books and blogs, and nowhere did I see illustrated graphs for the NLP data set. \n",
        "\n",
        "- <h2>Why not?</h2>\n",
        "\n",
        "- Wallaby and I are not shy from original thoughts, so we draw graphs after graphs. We used Monty’s ability to draw 2D diagrams and 3D charts, and Wallaby found what she is hoping to find, an original thought. The below is a cleanup code-cell version. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sK-Ve3MiMS1"
      },
      "source": [
        "# return, tokens-count, unique token-count, unknown token-count\n",
        "@add_method(AUD3)\n",
        "def _fetch_token_info(self, token_arr):\n",
        "  i = len(token_arr)\n",
        "  j = len(numpy.unique(token_arr))\n",
        "  k = i - numpy.count_nonzero(token_arr)\n",
        "  return i, j, k\n",
        "#\n",
        "#\n",
        "#\n",
        "@add_method(AUD3)\n",
        "def fetch_token_graph_data(self):\n",
        "  # set up\n",
        "  t = self._nlp_3of6.train.x.items\n",
        "  self.train_count = len(t)\n",
        "  self.train_graph_data = numpy.ones((self.train_count,3))\n",
        "  v = self._nlp_3of6.valid.x.items\n",
        "  self.valid_count = len(v)\n",
        "  self.valid_graph_data = numpy.ones((self.valid_count,3))\n",
        "  for i in range(self.train_count):\n",
        "    a,b,c = self._fetch_token_info(t[i])\n",
        "    self.train_graph_data[i,:] = [a, b, c]\n",
        "  #\n",
        "  for i in range(self.valid_count):\n",
        "    a,b,c = self._fetch_token_info(v[i])\n",
        "    self.valid_graph_data[i,:] = [a, b, c]\n",
        "  #\n",
        "  return\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKVdqWhskiMi"
      },
      "source": [
        "# do it\n",
        "Wallaby.fetch_token_graph_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ0zBr0C-mk9"
      },
      "source": [
        "# draw the train, test, valid data set\n",
        "@add_method(AUD3)\n",
        "def draw_tokenizer(self):\n",
        "  # set up\n",
        "  mx_train_token = numpy.ones((self.train_count,2))\n",
        "  mx_train_unique = numpy.ones((self.train_count,2))\n",
        "  mx_train_discard = numpy.ones((self.train_count,2))\n",
        "  #\n",
        "  mx_train_token[:,0] = numpy.arange(0, self.train_count)\n",
        "  mx_train_token[:,1] = self.train_graph_data[:,0]\n",
        "  mx_train_token[:,1].sort()\n",
        "  #\n",
        "  mx_train_unique[:,0] = numpy.arange(0, self.train_count)\n",
        "  mx_train_unique[:,1] = self.train_graph_data[:,1]\n",
        "  mx_train_unique[:,1].sort()\n",
        "  #\n",
        "  mx_train_discard[:,0] = numpy.arange(0, self.train_count)\n",
        "  mx_train_discard[:,1] = self.train_graph_data[:,2]\n",
        "  mx_train_discard[:,1].sort()\n",
        "  #\n",
        "  #\n",
        "  mx_valid_token = numpy.ones((self.valid_count,2))\n",
        "  mx_valid_unique = numpy.ones((self.valid_count,2))\n",
        "  mx_valid_discard = numpy.ones((self.valid_count,2))\n",
        "  #\n",
        "  mx_valid_token[:,0] = numpy.arange(0, self.valid_count)\n",
        "  mx_valid_token[:,1] = self.valid_graph_data[:,0]\n",
        "  mx_valid_token[:,1].sort()\n",
        "  #\n",
        "  mx_valid_unique[:,0] = numpy.arange(0, self.valid_count)\n",
        "  mx_valid_unique[:,1] = self.valid_graph_data[:,1]\n",
        "  mx_valid_unique[:,1].sort()\n",
        "  #\n",
        "  mx_valid_discard[:,0] = numpy.arange(0, self.valid_count)\n",
        "  mx_valid_discard[:,1] = self.valid_graph_data[:,2]\n",
        "  mx_valid_discard[:,1].sort()\n",
        "  #\n",
        "  # draw area/line graph\n",
        "  frame, pic = monty.fetch.graph_canvas(row=1,col=2,size=(18,9))\n",
        "  monty.draw.graph_line(pic[0], mx_train_token,is_shade_area=True, shade_alpha=0.5, shade_color=monty.bag.color.teal)\n",
        "  monty.draw.graph_line(pic[0], mx_train_unique,is_shade_area=True, shade_alpha=0.5, shade_color=monty.bag.color.yellow)\n",
        "  monty.draw.graph_line(pic[0], mx_train_discard,is_shade_area=True, shade_alpha=0.5, shade_color=monty.bag.color.red)\n",
        "  #\n",
        "  monty.draw.graph_line(pic[1], mx_valid_token,is_shade_area=True, shade_alpha=0.5, shade_color=monty.bag.color.teal)\n",
        "  monty.draw.graph_line(pic[1], mx_valid_unique,is_shade_area=True, shade_alpha=0.5, shade_color=monty.bag.color.yellow)\n",
        "  monty.draw.graph_line(pic[1], mx_valid_discard,is_shade_area=True, shade_alpha=0.5, shade_color=monty.bag.color.red)\n",
        "  # label them\n",
        "  xlab = \"Movie Reviews Sorted By Token Count\"\n",
        "  ylab = \"Token Count (Teal), Unique (Yellow/Green), Discard (Red)\"\n",
        "  monty.draw.graph_label(pic[0],xlabel=xlab, ylabel=ylab, head=\"Tokenizer, Training Set\")\n",
        "  monty.draw.graph_label(pic[1],xlabel=xlab, ylabel=ylab, head=\"Tokenizer, Validation Set\")\n",
        "  frame.show()\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-N2c5mdljJ6"
      },
      "source": [
        "# do it\n",
        "Wallaby.draw_tokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyCr1hS419ar"
      },
      "source": [
        "- They are beautiful graphs. With a glance, we verify the tokenizer is working magnificently, and the people at the IMDB curated a perfect set of NLP data. \n",
        "\n",
        "- When you are working on your NLP project, the NLP data should look like these two graphs. It’s so well normalized. Wallaby is ready to re-use this graphing method for all of her NLP projects. \n",
        "\n",
        "- Wallaby was so worried about the “63% discarded tokens,” and the tokens count (163,207) is twice as many as the IMDB’s vocab files (89,527). \n",
        "\n",
        "- The graphs show the “discarded token” per file is tiny. It’s barely registered on the chart. Therefore, it does not matter how many total tokens we discarded, as long as the per file per discard token is low. \n",
        "\n",
        "- We will ask Wallaby to graph the average per file count to verify the point above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bhQG7tRvrAS"
      },
      "source": [
        "# draw tokenizer average\n",
        "@add_method(AUD3)\n",
        "def draw_tokenizer_average(self):\n",
        "  # set up\n",
        "  mx_data = numpy.ones((3,2))\n",
        "  mx_data[:,0] = numpy.arange(1,4)\n",
        "  i = (self.train_graph_data[:,0].mean() + self.valid_graph_data[:,0].mean()) / 2\n",
        "  mx_data[0,1] = i\n",
        "  #\n",
        "  j = (self.train_graph_data[:,1].mean() + self.valid_graph_data[:,1].mean()) / 2\n",
        "  mx_data[1,1] = j\n",
        "  #\n",
        "  k = (self.train_graph_data[:,2].mean() + self.valid_graph_data[:,2].mean()) / 2\n",
        "  mx_data[2,1] = k\n",
        "  # draw bar chart\n",
        "  frame, pic = monty.fetch.graph_canvas()\n",
        "  monty.draw.graph_bar(pic,mx_data, is_hand=True,color=monty.bag.color.orange)\n",
        "  xlab = \"Token (\" + str(round(i,2)) + \"), Unique (\" + str(round(j,2)) + \"), Discarded (\" + str(round(k,2)) + \")\"\n",
        "  monty.draw.graph_label(pic,xlabel=xlab, ylabel=\"Token Count\", head=\"Tokenizer, Average Token Per File\")\n",
        "  frame.show()\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFj1Y0Cxv3tF"
      },
      "source": [
        "# do it\n",
        "Wallaby.draw_tokenizer_average()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HupcIl6FY_Gu"
      },
      "source": [
        "- The “Tokenizer Average” graph proves it. The tokenizer is working correctly, and the default 60,000 maximum token buffer is sufficient. \n",
        "\n",
        "- Wallaby doesn't need to increase the maximum token buffer or change the frequency of words from twice to three-time before accepting it as a valid token. \n",
        "\n",
        "- On average, for a movie review of 297 words, the system only discarded 1.64 words. That is 0.55% per file. \n",
        "\n",
        "- At first look, there are 63% token discarded. Wallaby thought that the system dropped every other word in a movie review, but the system only dropped one or two words in actuality.\n",
        "\n",
        "- The “Tokenizer graphs” count as an “original thought.” Therefore, Wallaby wins 1,000 gold coins. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk2yORQOdiB9"
      },
      "source": [
        "><center><h2><i>Wallaby has an original thought, Yippy!</i></h2></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnldMDiEhDFY"
      },
      "source": [
        "### 2.6.3D -- Normalize Token Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXqLHPIE02Cp"
      },
      "source": [
        "- Moving forward, we will ask Wallaby to do the following task.\n",
        "\n",
        "1. Teach Wallaby to add spaces before and after “period, comma, and dash.”\n",
        "\n",
        "1. We could hack the Fast.ai libraries, but that would take a bit longer, and Wallaby, with her tail wagging, is so eager to help, so Wallaby, go for it.\n",
        "\n",
        "3. Re-run the first three steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS5ahm0eQHSC"
      },
      "source": [
        "import re\n",
        "#\n",
        "# \n",
        "# clean glob words\n",
        "@add_method(AUD3)\n",
        "def _clean_glob_line(self, line):\n",
        "  return re.sub(r'(?<=[.,-])(?=[^\\s])', r' ', line)\n",
        "#\n",
        "#\n",
        "# clean glob in file\n",
        "@add_method(AUD3)\n",
        "def _clean_glob_file(self, original_file, _clean_function):\n",
        "  src = \"_tmp1__.txt\"\n",
        "  is_ok = True\n",
        "  with open(original_file) as old, open(src, 'w') as new:\n",
        "    for line in old:\n",
        "      a = _clean_function(line)\n",
        "      new.write(a)\n",
        "  new.close()\n",
        "  old.close()\n",
        "  os.rename(src, original_file)\n",
        "  return is_ok\n",
        "#\n",
        "# -------------------------------------- + ----------------------------------------\n",
        "#\n",
        "@add_method(AUD3)\n",
        "def clean_glob_dir(self, src_dir,_clean_function):\n",
        "  is_ok = True\n",
        "  i = 0\n",
        "  self._ph()\n",
        "  if (os.path.isdir(src_dir)):\n",
        "    for root, dirs, files in os.walk(src_dir):  # @UnusedVariable\n",
        "      for name in files:\n",
        "        a = pathlib.Path(root, name)\n",
        "        self._clean_glob_file(a, _clean_function)\n",
        "        i = i + 1\n",
        "    self._pp(\"Total files count\", i)\n",
        "  else:\n",
        "    self._pp(\"**Error not a directory\", src_dir)\n",
        "    is_ok = False\n",
        "  self._ph()\n",
        "  return is_ok\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYb25UL66MXs"
      },
      "source": [
        "# do it\n",
        "Wallaby.clean_glob_dir(pathlib.Path(\"data/imdb\"), Wallaby._clean_glob_line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLOmOsbpwq-u"
      },
      "source": [
        "- Wallaby, good job!\n",
        "\n",
        "- Re-run the first 3 steps of the data-bunch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aggzPJhdhtiU"
      },
      "source": [
        "### 2.6.1 (Rerun) Read the data and inspect it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3A2p7QHxAHM"
      },
      "source": [
        "# do it\n",
        "Wallaby.fetch_1of6_read_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuD5EhBgjdWm"
      },
      "source": [
        "- It is the same result as the first time. The good news is that Wallaby has not broken anything."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARawAR6_jMWc"
      },
      "source": [
        "### 2.6.2 (Rerun) Split the “train” and “valid” set and inspect it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaGPR3iixAML"
      },
      "source": [
        "# do it\n",
        "Wallaby.fetch_2of6_split_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V9OQjljjXrQ"
      },
      "source": [
        "- It is the same result as the first time. Good job, Wallaby."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G2dMsQpkRQ2"
      },
      "source": [
        "### 2.6.3 (Rerun) Label and tokenize the data and inspect it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwHshuCBxAQF"
      },
      "source": [
        "# do it\n",
        "Wallaby.fetch_3of6_label_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5A7Y5XRxBRG"
      },
      "source": [
        "#print random unknown, unused words, run it a dozen of time or more.\n",
        "Wallaby.print_random_unused_words()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5oo4TXjxBV5"
      },
      "source": [
        "#print random unknown, unused words, run it a dozen of time or more.\n",
        "Wallaby.print_random_unused_words(is_reversed=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbgcEMrZsYwf"
      },
      "source": [
        "- From the above results, in section #3, the total tokens were reduced to 156,091. That is a 7,116 tokens reduction. Good job, Wallaby!\n",
        "\n",
        "- In section #4, the total discarded token percentage reduced from 63% to 62%. It’s a small step in the right direction, but as the “Tokenizer” graphs show, the discarded tokens per file are more weighty.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tvwlUjp6MQt"
      },
      "source": [
        "# do it\n",
        "Wallaby.fetch_token_graph_data()\n",
        "Wallaby.draw_tokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wtj-RrlC7HVE"
      },
      "source": [
        "# do it\n",
        "Wallaby.draw_tokenizer_average()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0CVKchps1R9"
      },
      "source": [
        "- It is amazing that with over 20 million data points, we can verify results with a glance. We didn’t break the tokenizer, and we improved it by the tiniest of margin. The average discard tokens per file are reducing from 1.64 tokens to 1.63 tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAypEsFCs42C"
      },
      "source": [
        "### 2.6.4 Add data augmentation and inspect it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hXFUpFYtI6A"
      },
      "source": [
        "- With the image classifier or image segmentation ANN model, Wallaby uses data augmentation with a notable effect. By flipping, skewing, and warping the images, she increases the training data set by a factor of four to twelve. \n",
        "\n",
        "- So why can’t Wallaby augments the writing words? In English, you can’t flip characters or words. Skewing and warping could not apply to words. \n",
        "\n",
        "- Wallaby doesn't know how or read any book or article about word-augmentation. She doesn't even know if it would be possible. She needs to consult a linguist. \n",
        "\n",
        "- Even if everyone said, “NO, it can’t be done.” Wallaby would not stop exploring “why” because it would be hubris to think that “if I can’t do it, no one else can.”\n",
        "\n",
        "- Wallaby could smell another adventure for the next “sandbox” project. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2ViFTEw7Hbt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOFc7XIhtNWD"
      },
      "source": [
        "### 2.6.5 Fetch the batch-size, the data-bunch, and inspect it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVeo6G7stps2"
      },
      "source": [
        "- Wallaby needs a better method of calculating batch-size. For now, Monty based it only on the available free GPU RAM. It should be a straightforward math equation of free GPU RAM, input data size, and the number of layers in the base architecture. \n",
        "\n",
        "- Wallaby did a quick online search, and she did not find any article or blog about a formal batch-size calculation. It would make a fun next “sandbox” project. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCyVhH9g7Hg1"
      },
      "source": [
        "# GPU and RAM info\n",
        "monty.print.gpu_info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WhCDX2H7HZh"
      },
      "source": [
        "# \n",
        "# data bunch\n",
        "@add_method(AUD3)\n",
        "def fetch_5of6_databunch(self):\n",
        "  self.batch_size = monty.fetch.batch_size()\n",
        "  self._nlp_5of6 = self._nlp_3of6.databunch(bs=self.batch_size)\n",
        "  monty.print.inspect_fastai_data_bunch(self._nlp_5of6)\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFZ43nwNjeut"
      },
      "source": [
        "# do it\n",
        "Wallaby.fetch_5of6_databunch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpiP57xUwjtE"
      },
      "source": [
        "- Once again, the Fast.ai library does all the work. Inspecting the data-bunch, section #1 shows that the train data-set is correct. Section #2 shows the valid data-set is accurate, and section #3 shows the batch-size is 48, and we are using the GPU-Cuda interface. \n",
        "\n",
        "- Section #4 shows the tokenizer is working reliably.\n",
        "\n",
        "- To double-checking the data-bunch, we use Fast.ai’s show_batch() method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APTna2STweos"
      },
      "source": [
        "Wallaby._nlp_5of6.show_batch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlGR5n9exOB9"
      },
      "source": [
        "### 2.6.6 Normalize the input-data for the selected base-architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQitgP1YxaKp"
      },
      "source": [
        "- Similar to the “data augmentation” discussion, Wallaby routinely tag on the normalization function to the data-bunch. For an image classifier project that uses the “resnet-34 or resnet-50” base architecture, she uses the “imagenet_stats” as an input to the normalization function. \n",
        "\n",
        "- The data-bunch normalization function is not the same as “NLP Normalization.” Some literature refers to “NLP Normalization” as transforming the words to a standard format, such as converting all words to lowercase.\n",
        "\n",
        "- One more time, “Why not?”\n",
        "\n",
        "- It is the four detours in the “AUD3” journey that Wallaby didn’t take. She will need a pack of friends for upcoming adventures.\n",
        "\n",
        "- That concludes the “next word” data-bunch. Wallaby will move forward to define the “sentiment prediction” data-bunch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SKlxKp2hdMn"
      },
      "source": [
        "## 2.8 Milestone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzPHMhDmiS4m"
      },
      "source": [
        "- Henna is happy to see the milestone mark. She has completed the AUD3 journey, except for a quick detour to hyper-parameters.\n",
        "\n",
        "- At the beginning of the journey, we promised to show Henna how the movie reviews affect the hyper-parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voQVxSK5iNp0"
      },
      "source": [
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng5utvGOz_z0"
      },
      "source": [
        "## 2.10 Wrap Up "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_i1z4XqTvcj"
      },
      "source": [
        "- We are back from the hyper-parameters detour, and Henna is happy heading back to the home base. Henna has been on a similar journey with real-world data. It was for a national retail customer feedback blog site. The difference is that Henna spends more time cleaning, labeling, augmenting, and segmenting customer feedback. In other words, it is not as normalized as the IMDB movie reviews.\n",
        "\n",
        "- Henna starts by downloading the IMDB movie reviews. She looks at the data structure, counting words, and drawing graphs. The combined word-counts and standard deviation charts in section #2.3 are very useful. \n",
        "\n",
        "- Henna moves forward to reading the movie reviews, the “README” file, and other supporting files. She found a few issues, such as HTML-tags and unprintable characters that need to be clean. \n",
        "\n",
        "- After cleaning the movie reviews, Henna splits the “labeled” data to “75-15-10”, i.e., 75% for the training, 15% for the validating, and 10% for the testing. The “75-15-10” split might disqualify Henna in a competition, but it is a balanced distribution for a real-world project. \n",
        "\n",
        "- Hanna did not cheat. She didn’t use the “validation” movie reviews for “training,” and the “test” data are kept separate. The “75-15-10” hand-drawn bar chart in section #2.5 confirmed that Henna did it correctly.\n",
        "\n",
        "- At first glance at the tokenizer result, Henna was panic because the system tokenizes 47% of all the available unique words. It implies the system dropped every other word in the movie reviews. \n",
        "\n",
        "- However, through Henna’s original thinking and graphs, the “tokenizer charts” in section #2.6.3C show that in actuality, for the average 232 words movie review, the system only discards 1.62 words. The “tokenizer charts” conclusively prove that the system discards 0.7% per movie review. \n",
        "\n",
        "- Henna discovered a few intentional biases in the movie reviews. In her biases discussion, she explains how to correct them and encourages data scientists to document their data’s intentional biases. \n",
        "\n",
        "- In the last part of the journey, Henna create the “next word” data-bunch and “sentiment classification” data-bunch. She inspects each step in the data-bunch creation. \n",
        "\n",
        "- It is an enjoyable journey, and Henna wishes you to be back for the next adventure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWwraVbmlgwL"
      },
      "source": [
        "## 2.11 Bonus Section, NLP Model Loss and Accuracy Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHLeT8ZBlvIH"
      },
      "source": [
        "- Henna was asked about the NLP model training accuracy results using her two data-bunches, so she asked “Spooky,” who is from the Jupiter notebook that does the model training. \n",
        "\n",
        "- Hanna happy to report that the final accuracy rate is 96.800%. Compared to the Kaggle IMDB competition about two years ago, the “[ml410-IMDb](https://www.kaggle.com/c/ml410-imdb/leaderboard)” Private Leaderboard is 90.528%.\n",
        "\n",
        "- Granted, Spooky is using the Fast.ai library, which included the last two years advances in CNN, but to beat the leader board by 6.272% is a substantial margin where Kaggle competition winners are separated tenth or hundredth of a percentage. \n",
        "\n",
        "- Spooky’s Jupyter notebook is not (yet) published on the public GitHub, but Henna has permission to includes the result here. \n",
        "\n",
        "- Using Panda, the raw data are as follows. The system recalculates the learning rate after each unfreezing-layers training session. In addition, Henna splits the clean-data into the “75-15-10” split. See section #2.5 above.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzD4tBrfmDOM"
      },
      "source": [
        "# read it\n",
        "henna.df_result = pandas.read_csv(\"spooky_result.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-h0e4-wnByS"
      },
      "source": [
        "# display it\n",
        "henna.df_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf1KhrtJncWE"
      },
      "source": [
        "- Henna likes to draw, so here is a beautiful Loss and Accuracy’s graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po_Dtde9nBu1"
      },
      "source": [
        "# draw loss and accuracy training graph\n",
        "@add_method(AUD3)\n",
        "def draw_loss_acu_result(self,is_logr=False):\n",
        "  try:\n",
        "    #set up train set\n",
        "    row = len(self.df_result)\n",
        "    mx_accuracy = numpy.ones((row,2))\n",
        "    mx_accuracy[:,0] = numpy.arange(0,row,1)\n",
        "    mx_accuracy[:,1] = self.df_result[\"accuracy\"]\n",
        "    # set up train_loss\n",
        "    mx_train_loss = mx_accuracy.copy()\n",
        "    mx_train_loss[:,1] = self.df_result[\"train_loss\"]\n",
        "    #\n",
        "    mx_valid_loss = mx_accuracy.copy()\n",
        "    mx_valid_loss[:,1] = self.df_result[\"valid_loss\"]\n",
        "    # \n",
        "    mx_1_line = numpy.zeros((2,2))\n",
        "    mx_1_line[0,1] = 1.0\n",
        "    mx_1_line[1,0] = row - 1\n",
        "    mx_1_line[1,1] = 1.0\n",
        "    # \n",
        "    #\n",
        "    # draw it\n",
        "    frame, pic = monty.fetch.graph_canvas()\n",
        "    monty.draw.graph_line(pic,mx_1_line,is_grid=True)\n",
        "    monty.draw.graph_line(pic,mx_accuracy,is_shade_area=True,shade_alpha=0.8,shade_color=monty.bag.color.yellow)\n",
        "    monty.draw.graph_line(pic,mx_valid_loss,is_shade_area=True,shade_alpha=0.8,shade_color=monty.bag.color.blue)\n",
        "    monty.draw.graph_line(pic,mx_train_loss,is_shade_area=True,shade_alpha=0.7,shade_color=monty.bag.color.teal)\n",
        "    #\n",
        "    #\n",
        "    x = \"Epoch\"\n",
        "    y = \"Loss and Accuracy\"\n",
        "    h = \"Accuracy (Yellow), Train Loss (Teal), Valid Loss (Blue)\"\n",
        "    xl = list(map(str, list(self.df_result[\"epoch\"])))\n",
        "    monty.draw.graph_label(pic,xlabel=x, ylabel=y, head=h)\n",
        "    pic.set_xticks(mx_accuracy[:,0])\n",
        "    pic.set_xticklabels(xl)\n",
        "    if (is_logr):\n",
        "      pic.set_yscale('log')\n",
        "    pic.grid(True)\n",
        "    frame.show()\n",
        "  except:\n",
        "    self._pp(\"**Error, can not draw graph\", \"Did you create my buddy, Monty?\")\n",
        "  return\n",
        "# set_xticklabels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXFpApsygqDQ"
      },
      "source": [
        "# do it\n",
        "henna.draw_loss_acu_result()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2icGloIkwbgW"
      },
      "source": [
        "# do it using logarithmic scale\n",
        "henna.draw_loss_acu_result(is_logr=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TflbirNOugWR"
      },
      "source": [
        "monty.draw.graph_line??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l589YYPUXZhP"
      },
      "source": [
        "><h2><center>The end.</center></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU-mFY1mcnPy"
      },
      "source": [
        "# 3 - Conclusion\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqhzMwbaVsy0"
      },
      "source": [
        "The “AUD3” is a typical Jupyter notebook in my workday. The difference is that the actual notebooks are messier, containing many detours, deadends, and mistakes.\n",
        "\n",
        "I choose the IMDB data because I can’t share the actual customer data, and I am pleasantly surprised how clean are the movie reviews. The IMDB folks have rightfully deserved the credits for making the NLP data freely available. \n",
        "\n",
        "I discovered that graphing NLP data is my secret weapon, especially the “tokenizer graph per file.” I can’t believe that no one does it before. \n",
        "\n",
        "Typically, I spend more time cleaning, augmenting, segmenting, and labeling the NLP data in a real-world project. Furthermore, the data scientists and project managers rarely document intentional biases. They are not hard to spot once you compared with the project objectives.\n",
        "\n",
        "As with the previous “sandbox” project, [the 3D visualizing](https://www.linkedin.com/pulse/python-3d-visualization-hackable-step-by-step-jupyter-duc-haba/), I encourage everyone should publish articles or lessons using the Jupyter notebook. The readers can hack the notebook and make it their own. That’s is where learning truly takes root. It is by reading and doing it.\n",
        "\n",
        "> <h2>“A doer of deeds…”</h2>\n",
        "\n",
        "I am looking forward to seeing you again in the next “sandbox” adventure, and if you read this on LinkedIn or GitHub, give me a “thumbs up” and send me feedback.\n",
        "\n",
        "- LinkedIn, \"Demystify Python 3D Visualization -- A Hackable Step-by-step Jupyter Notebook\", (add link)\n",
        "\n",
        "- If you read this on LinkedIn, what are you waiting for? Heading over to Github, using Google Collab or your favorite Jupyter notebook option, and hacking away. (add link)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEvXCZWiSejW"
      },
      "source": [
        "# end of jupyter notebook"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}